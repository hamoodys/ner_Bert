{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"-z2N4tJk-o5B","outputId":"8feab305-4f13-4caa-9cd0-9aed56de3c50","execution":{"iopub.status.busy":"2022-05-25T23:34:26.097054Z","iopub.execute_input":"2022-05-25T23:34:26.097981Z","iopub.status.idle":"2022-05-25T23:34:37.715474Z","shell.execute_reply.started":"2022-05-25T23:34:26.097831Z","shell.execute_reply":"2022-05-25T23:34:37.714564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imports \nimport numpy as np\nimport pandas as pd\nimport re\nimport torch\n\nfrom transformers import BertTokenizerFast, BertForTokenClassification\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom torch.optim import SGD","metadata":{"id":"HKIW7_B2f7-R","execution":{"iopub.status.busy":"2022-05-25T23:34:37.718481Z","iopub.execute_input":"2022-05-25T23:34:37.718761Z","iopub.status.idle":"2022-05-25T23:34:44.999029Z","shell.execute_reply.started":"2022-05-25T23:34:37.718722Z","shell.execute_reply":"2022-05-25T23:34:44.998175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Constructing NER dataset**","metadata":{"id":"M_6_YNSzCw0Q"}},{"cell_type":"code","source":"df = pd.read_csv('../input/homeworkexercise/Homework Exercise.csv')","metadata":{"id":"f0-gxCQkgB5I","execution":{"iopub.status.busy":"2022-05-25T23:34:45.000324Z","iopub.execute_input":"2022-05-25T23:34:45.000598Z","iopub.status.idle":"2022-05-25T23:34:45.025618Z","shell.execute_reply.started":"2022-05-25T23:34:45.00056Z","shell.execute_reply":"2022-05-25T23:34:45.024837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"K4FzcsqggH_Q","outputId":"c82e8524-b9b6-4540-9ce4-6f6d0795df4e","execution":{"iopub.status.busy":"2022-05-25T23:34:45.028349Z","iopub.execute_input":"2022-05-25T23:34:45.028837Z","iopub.status.idle":"2022-05-25T23:34:45.051343Z","shell.execute_reply.started":"2022-05-25T23:34:45.028796Z","shell.execute_reply":"2022-05-25T23:34:45.050607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fucntion to construct Named Entity Recognition Labels for the given store no. task\ndef construct_ner_labels(sentence, label):\n  ner_labels = []\n  s = sentence.split(\" \")\n\n  for index, word in enumerate(s):\n        number = re.findall(r'\\d+', word)\n        match = re.findall(r'\\d+', label)\n        if(len(number) > 0):\n          if (match[0] == number[0].lstrip('0')):\n            ner_labels.append('B-sto')\n          else:\n            ner_labels.append('O')\n        else: \n          ner_labels.append('O')\n  \n  return ner_labels","metadata":{"id":"IU0a4FyX4LZF","execution":{"iopub.status.busy":"2022-05-25T23:37:10.983979Z","iopub.execute_input":"2022-05-25T23:37:10.984577Z","iopub.status.idle":"2022-05-25T23:37:10.991645Z","shell.execute_reply.started":"2022-05-25T23:37:10.984536Z","shell.execute_reply":"2022-05-25T23:37:10.990907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"des = df.iloc[295]['transaction_descriptor']\nlabel = df.iloc[295]['store_number']","metadata":{"id":"C4zLhfBRgIot","execution":{"iopub.status.busy":"2022-05-25T23:37:13.966482Z","iopub.execute_input":"2022-05-25T23:37:13.966768Z","iopub.status.idle":"2022-05-25T23:37:13.97527Z","shell.execute_reply.started":"2022-05-25T23:37:13.966733Z","shell.execute_reply":"2022-05-25T23:37:13.97446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"des, label","metadata":{"id":"tm0gbcGp79tA","outputId":"96df7dbd-31f0-405f-ab2d-b988f4fa400e","execution":{"iopub.status.busy":"2022-05-25T05:45:40.698917Z","iopub.execute_input":"2022-05-25T05:45:40.699222Z","iopub.status.idle":"2022-05-25T05:45:40.708781Z","shell.execute_reply.started":"2022-05-25T05:45:40.699185Z","shell.execute_reply":"2022-05-25T05:45:40.707999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"construct_ner_labels(des, label)  # We denote the store entity by B-sto tag ","metadata":{"id":"zV-OgUh671z1","outputId":"a95f78c3-e35f-470c-89f2-83d66a57a9ee","execution":{"iopub.status.busy":"2022-05-25T23:37:19.509325Z","iopub.execute_input":"2022-05-25T23:37:19.509949Z","iopub.status.idle":"2022-05-25T23:37:19.516293Z","shell.execute_reply.started":"2022-05-25T23:37:19.509891Z","shell.execute_reply":"2022-05-25T23:37:19.51545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# construct a label list \nlabel_list = []\n\nfor index, row in df.iterrows():\n  sen, label = row['transaction_descriptor'], row['store_number']\n  ner_labels = construct_ner_labels(sen, label)\n  label_list.append(' '.join(ner_labels))","metadata":{"id":"iyJtC9id57Lw","execution":{"iopub.status.busy":"2022-05-25T23:40:51.203128Z","iopub.execute_input":"2022-05-25T23:40:51.203604Z","iopub.status.idle":"2022-05-25T23:40:51.238392Z","shell.execute_reply.started":"2022-05-25T23:40:51.203558Z","shell.execute_reply":"2022-05-25T23:40:51.236141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_list","metadata":{"id":"2jBMJElK7sfh","outputId":"e2ba2380-8291-4e6b-f6dc-974d541da17f","execution":{"iopub.status.busy":"2022-05-25T23:41:03.281469Z","iopub.execute_input":"2022-05-25T23:41:03.281782Z","iopub.status.idle":"2022-05-25T23:41:03.294072Z","shell.execute_reply.started":"2022-05-25T23:41:03.28175Z","shell.execute_reply":"2022-05-25T23:41:03.292945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# construct a label column \ndf['labels'] = label_list","metadata":{"id":"ko5euJHd7tf8","execution":{"iopub.status.busy":"2022-05-25T23:41:22.365989Z","iopub.execute_input":"2022-05-25T23:41:22.366538Z","iopub.status.idle":"2022-05-25T23:41:22.371625Z","shell.execute_reply.started":"2022-05-25T23:41:22.366499Z","shell.execute_reply":"2022-05-25T23:41:22.370519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the dataframes according to dataset column\ndf_train = df[df['dataset'] == 'train']\ndf_val = df[df['dataset'] == 'validation']\ndf_test = df[df['dataset'] == 'test']","metadata":{"id":"ln1PH4A_82dK","execution":{"iopub.status.busy":"2022-05-25T23:41:27.610808Z","iopub.execute_input":"2022-05-25T23:41:27.611496Z","iopub.status.idle":"2022-05-25T23:41:27.620822Z","shell.execute_reply.started":"2022-05-25T23:41:27.611456Z","shell.execute_reply":"2022-05-25T23:41:27.619816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# restting the indices\ndf_train = df_train.reset_index()\ndf_val = df_val.reset_index()\ndf_test = df_test.reset_index()","metadata":{"id":"054B51auD85a","execution":{"iopub.status.busy":"2022-05-25T23:41:50.591181Z","iopub.execute_input":"2022-05-25T23:41:50.59148Z","iopub.status.idle":"2022-05-25T23:41:50.599009Z","shell.execute_reply.started":"2022-05-25T23:41:50.591449Z","shell.execute_reply":"2022-05-25T23:41:50.598183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"id":"KivfzgTn822e","outputId":"8be651a4-afce-426d-c297-ce67e5db6962","execution":{"iopub.status.busy":"2022-05-25T23:42:01.788979Z","iopub.execute_input":"2022-05-25T23:42:01.789269Z","iopub.status.idle":"2022-05-25T23:42:01.804865Z","shell.execute_reply.started":"2022-05-25T23:42:01.789237Z","shell.execute_reply":"2022-05-25T23:42:01.804201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Pre-processing the dataset for NER task**","metadata":{"id":"68o0X57LEHCv"}},{"cell_type":"code","source":"# get all the labels \nlabels = [labs.split() for labs in df_train['labels'].values.tolist()]\nlabels[:5]","metadata":{"id":"c7egQ-Hc-MHm","outputId":"36175551-977b-4488-cae0-436ba3d2dfc0","execution":{"iopub.status.busy":"2022-05-25T23:43:24.405709Z","iopub.execute_input":"2022-05-25T23:43:24.406314Z","iopub.status.idle":"2022-05-25T23:43:24.413499Z","shell.execute_reply.started":"2022-05-25T23:43:24.406276Z","shell.execute_reply":"2022-05-25T23:43:24.412691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many labels are there in the dataset\n#unique_labels = set()\n\n#for lb in labels:\n  #[unique_labels.add(i) for i in lb if i not in unique_labels]\n\n# Print the unique labels \n#print(unique_labels)\n# There are only two unique labels here blank and store_no. ","metadata":{"id":"z08bbL8UEfUt","outputId":"666dd981-816c-4c77-c106-46672e58af51","execution":{"iopub.status.busy":"2022-05-25T23:46:14.417785Z","iopub.execute_input":"2022-05-25T23:46:14.418511Z","iopub.status.idle":"2022-05-25T23:46:14.422649Z","shell.execute_reply.started":"2022-05-25T23:46:14.418473Z","shell.execute_reply":"2022-05-25T23:46:14.421628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map each label into its index representation and vice versa\nlabels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\nids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n\n# print a two way mapping \nprint(labels_to_ids)\nprint(ids_to_labels)","metadata":{"id":"R55clP3YEgGR","outputId":"24428056-6f6c-4718-8e7b-ae081dcbf2ae","execution":{"iopub.status.busy":"2022-05-25T23:46:16.463639Z","iopub.execute_input":"2022-05-25T23:46:16.464225Z","iopub.status.idle":"2022-05-25T23:46:16.47027Z","shell.execute_reply.started":"2022-05-25T23:46:16.464186Z","shell.execute_reply":"2022-05-25T23:46:16.469319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Tokenizing the data**","metadata":{"id":"QzXkjQ7_FbLQ"}},{"cell_type":"code","source":"# We use HuggingFace library for tokenizer and BERT model \ntokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')","metadata":{"id":"2eG1L_OI-ZE6","execution":{"iopub.status.busy":"2022-05-25T23:44:05.866454Z","iopub.execute_input":"2022-05-25T23:44:05.867355Z","iopub.status.idle":"2022-05-25T23:44:07.3729Z","shell.execute_reply.started":"2022-05-25T23:44:05.867311Z","shell.execute_reply":"2022-05-25T23:44:07.372112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = df_train.iloc[0]['transaction_descriptor']\ntext_tokenized = tokenizer(sample, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")","metadata":{"id":"yMsE6k72_HqH","execution":{"iopub.status.busy":"2022-05-25T23:44:12.37295Z","iopub.execute_input":"2022-05-25T23:44:12.373268Z","iopub.status.idle":"2022-05-25T23:44:12.387742Z","shell.execute_reply.started":"2022-05-25T23:44:12.373238Z","shell.execute_reply":"2022-05-25T23:44:12.386929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text_tokenized","metadata":{"id":"darBpyMK_O-M","outputId":"93e1b994-30eb-4ac5-f3ff-15c2201d30a0","execution":{"iopub.status.busy":"2022-05-25T23:44:57.499829Z","iopub.execute_input":"2022-05-25T23:44:57.50051Z","iopub.status.idle":"2022-05-25T23:44:57.503943Z","shell.execute_reply.started":"2022-05-25T23:44:57.500473Z","shell.execute_reply":"2022-05-25T23:44:57.503054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word ids are useful for identifying various pieces of the word \n# This is necessary as we need to adjust the ner labels according to pieces  \n\nword_ids = text_tokenized.word_ids()\nprint(sample)\nprint(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\nprint(word_ids)","metadata":{"id":"yfHqHHZb_g8z","outputId":"3f9f4894-da26-4842-af1c-1d739ddfce0a","execution":{"iopub.status.busy":"2022-05-25T23:46:35.770605Z","iopub.execute_input":"2022-05-25T23:46:35.770872Z","iopub.status.idle":"2022-05-25T23:46:35.782354Z","shell.execute_reply.started":"2022-05-25T23:46:35.770843Z","shell.execute_reply":"2022-05-25T23:46:35.78152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a function that encapsulates the process of adjusting labels \n# We either fill all the pieces with same label or we only fill first sub word with label and rest with -100 \n# -100 is used as pytorch doesnot calculate the loss for this token \n\ndef align_label(texts, labels, label_all_tokens = False):\n    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n\n    word_ids = tokenized_inputs.word_ids()\n\n    previous_word_idx = None\n    label_ids = []\n\n    for word_idx in word_ids:\n\n        if word_idx is None:\n            label_ids.append(-100)\n\n        elif word_idx != previous_word_idx:\n            try:\n                label_ids.append(labels_to_ids[labels[word_idx]])\n            except:\n                label_ids.append(-100)\n        else:\n            try:\n                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n            except:\n                label_ids.append(-100)\n        previous_word_idx = word_idx\n\n    return label_ids","metadata":{"id":"xDIAP0D5GITg","execution":{"iopub.status.busy":"2022-05-25T23:54:49.014546Z","iopub.execute_input":"2022-05-25T23:54:49.014826Z","iopub.status.idle":"2022-05-25T23:54:49.022495Z","shell.execute_reply.started":"2022-05-25T23:54:49.014795Z","shell.execute_reply":"2022-05-25T23:54:49.021768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = labels[0]\n\nprint(f'label : {label}\\n')\n\n# set label all tokens to True \nlabel_all_tokens = True\n\nnew_label = align_label(sample, label, label_all_tokens)\n\nprint(f'label is assigned to all sub words : ')\nprint(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\nprint(new_label)\n\nprint()\n\n# set label all tokens to False\nlabel_all_tokens = False\n\nnew_label = align_label(sample, label, label_all_tokens)\n\nprint(f'label is assigned to first sub word only : ')\nprint(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\nprint(new_label)","metadata":{"id":"LX-JR29C_ADR","outputId":"e6ca6944-dd35-4fbb-e62b-b34bb0f35e08","execution":{"iopub.status.busy":"2022-05-25T23:54:51.680403Z","iopub.execute_input":"2022-05-25T23:54:51.681038Z","iopub.status.idle":"2022-05-25T23:54:51.698355Z","shell.execute_reply.started":"2022-05-25T23:54:51.680998Z","shell.execute_reply":"2022-05-25T23:54:51.697586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Constructing a Pytorch Dataset**","metadata":{"id":"umt4J1ajAjzN"}},{"cell_type":"code","source":"class NERDataset(torch.utils.data.Dataset):\n\n    def __init__(self, df):\n        lb = [i.split() for i in df['labels'].values.tolist()]\n        txt = df['transaction_descriptor'].values.tolist()\n        self.texts = [tokenizer(str(i),\n                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_data(self, idx):\n        return self.texts[idx]\n\n    def get_batch_labels(self, idx):\n        return torch.LongTensor(self.labels[idx])\n\n    def __getitem__(self, idx):\n        batch_data = self.get_batch_data(idx)\n        batch_labels = self.get_batch_labels(idx)\n\n        return batch_data, batch_labels","metadata":{"id":"SLsMcfTCAIiG","execution":{"iopub.status.busy":"2022-05-25T23:55:35.77331Z","iopub.execute_input":"2022-05-25T23:55:35.77358Z","iopub.status.idle":"2022-05-25T23:55:35.781966Z","shell.execute_reply.started":"2022-05-25T23:55:35.773551Z","shell.execute_reply":"2022-05-25T23:55:35.781235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model Definition**","metadata":{"id":"HkjVo5wHPyV8"}},{"cell_type":"code","source":"# BERT model for Named Entity Recognition : This is basically a model for token classification \nclass BertNER(torch.nn.Module):\n\n    def __init__(self, num_labels):\n        super(BertNER, self).__init__()\n        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n\n    def forward(self, input_id, mask, label):\n        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n        return output","metadata":{"id":"59fI9YMHAqQC","execution":{"iopub.status.busy":"2022-05-26T00:01:24.785829Z","iopub.execute_input":"2022-05-26T00:01:24.786458Z","iopub.status.idle":"2022-05-26T00:01:24.792457Z","shell.execute_reply.started":"2022-05-26T00:01:24.786418Z","shell.execute_reply":"2022-05-26T00:01:24.791736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertNER(len(unique_labels))","metadata":{"id":"j3DfOYuXMrf5","outputId":"8a808542-fda9-45fd-9829-a1e47895550b","execution":{"iopub.status.busy":"2022-05-26T00:01:28.703525Z","iopub.execute_input":"2022-05-26T00:01:28.704519Z","iopub.status.idle":"2022-05-26T00:01:45.289928Z","shell.execute_reply.started":"2022-05-26T00:01:28.704472Z","shell.execute_reply":"2022-05-26T00:01:45.28906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training**","metadata":{"id":"tlSvTPRGP1Qt"}},{"cell_type":"code","source":"# Hyperparams\nlr = 1e-3\nepochs = 10\nuse_cuda = torch.cuda.is_available()","metadata":{"id":"hKyHy8DxNBWN","execution":{"iopub.status.busy":"2022-05-26T00:01:45.292001Z","iopub.execute_input":"2022-05-26T00:01:45.292324Z","iopub.status.idle":"2022-05-26T00:01:45.356014Z","shell.execute_reply.started":"2022-05-26T00:01:45.292284Z","shell.execute_reply":"2022-05-26T00:01:45.354946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set device to 'cuda' if available\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")","metadata":{"id":"rXh2p8dmM9bO","execution":{"iopub.status.busy":"2022-05-26T00:01:45.357972Z","iopub.execute_input":"2022-05-26T00:01:45.358513Z","iopub.status.idle":"2022-05-26T00:01:45.365837Z","shell.execute_reply.started":"2022-05-26T00:01:45.358473Z","shell.execute_reply":"2022-05-26T00:01:45.364809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Construct the dataset and dataloaders \ntrain_dataset = NERDataset(df_train)\nval_dataset = NERDataset(df_val)\n\ntrain_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=1, shuffle=True)\nval_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=1)","metadata":{"id":"8Wuq0co1MybF","execution":{"iopub.status.busy":"2022-05-26T00:02:09.540967Z","iopub.execute_input":"2022-05-26T00:02:09.541547Z","iopub.status.idle":"2022-05-26T00:02:09.716175Z","shell.execute_reply.started":"2022-05-26T00:02:09.541508Z","shell.execute_reply":"2022-05-26T00:02:09.714257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for i,j in val_dataloader:\n  #print(i,j)","metadata":{"id":"azMzmdAaPMCo","outputId":"a5264ff6-4799-4561-d279-881d84ca0e5f","execution":{"iopub.status.busy":"2022-05-26T00:02:34.9481Z","iopub.execute_input":"2022-05-26T00:02:34.948402Z","iopub.status.idle":"2022-05-26T00:02:34.952134Z","shell.execute_reply.started":"2022-05-26T00:02:34.948372Z","shell.execute_reply":"2022-05-26T00:02:34.951167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We use SGD optimizer, we can use Adam or something else too \noptimizer = SGD(model.parameters(), lr=lr)","metadata":{"id":"ZYx5fMGzM_ej","execution":{"iopub.status.busy":"2022-05-26T00:02:41.888328Z","iopub.execute_input":"2022-05-26T00:02:41.888613Z","iopub.status.idle":"2022-05-26T00:02:41.894472Z","shell.execute_reply.started":"2022-05-26T00:02:41.888582Z","shell.execute_reply":"2022-05-26T00:02:41.89355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if use_cuda:\n  model = model.cuda()","metadata":{"id":"XHYb8S2rNg5T","execution":{"iopub.status.busy":"2022-05-26T00:18:09.90815Z","iopub.execute_input":"2022-05-26T00:18:09.908454Z","iopub.status.idle":"2022-05-26T00:18:09.91873Z","shell.execute_reply.started":"2022-05-26T00:18:09.908421Z","shell.execute_reply":"2022-05-26T00:18:09.917565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_acc = 0\nbest_loss = 1000\n\nfor epoch_num in range(epochs):\n    \n    #######################################################\n    ######################Training#########################\n    #######################################################\n\n    total_acc_train = 0\n    total_loss_train = 0\n\n    model.train()\n\n    for train_data, train_label in tqdm(train_dataloader):\n\n        train_label = train_label[0].to(device)\n        mask = train_data['attention_mask'][0].to(device)\n        input_id = train_data['input_ids'][0].to(device)\n\n        optimizer.zero_grad()\n        loss, logits = model(input_id, mask, train_label)\n\n        logits_clean = logits[0][train_label != -100]\n        label_clean = train_label[train_label != -100]\n\n        predictions = logits_clean.argmax(dim=1)\n\n        acc = (predictions == label_clean).float().mean()\n        total_acc_train += acc\n        total_loss_train += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n    #######################################################\n    ######################Validation#######################\n    #######################################################\n    \n    model.eval()\n\n    total_acc_val = 0\n    total_loss_val = 0\n\n    for val_data, val_label in val_dataloader:\n\n        val_label = val_label[0].to(device)\n        mask = val_data['attention_mask'][0].to(device)\n\n        input_id = val_data['input_ids'][0].to(device)\n\n        loss, logits = model(input_id, mask, val_label)\n\n        logits_clean = logits[0][val_label != -100]\n        label_clean = val_label[val_label != -100]\n\n        predictions = logits_clean.argmax(dim=1)          \n\n        acc = (predictions == label_clean).float().mean()\n        total_acc_val += acc\n        total_loss_val += loss.item()\n\n    val_accuracy = total_acc_val / len(df_val)\n    val_loss = total_loss_val / len(df_val)\n\n    print(\n    f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')","metadata":{"id":"Cq9rAUchA49-","outputId":"09d3e6a3-c4f1-42d8-f814-9dfe82de5b69","execution":{"iopub.status.busy":"2022-05-26T00:21:34.306069Z","iopub.execute_input":"2022-05-26T00:21:34.306427Z","iopub.status.idle":"2022-05-26T00:23:17.463589Z","shell.execute_reply.started":"2022-05-26T00:21:34.306388Z","shell.execute_reply":"2022-05-26T00:23:17.46255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Evaluation on Test Dataset**","metadata":{"id":"7tFAERoxOV5d"}},{"cell_type":"code","source":"test_dataset = NERDataset(df_test)\ntest_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)","metadata":{"id":"5SGgxMkuDGS0","execution":{"iopub.status.busy":"2022-05-26T00:23:32.740687Z","iopub.execute_input":"2022-05-26T00:23:32.741209Z","iopub.status.idle":"2022-05-26T00:23:32.867324Z","shell.execute_reply.started":"2022-05-26T00:23:32.741161Z","shell.execute_reply":"2022-05-26T00:23:32.866529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting labels over the test dataset \nmodel.eval()\n\npredicted_labels = []\n\nfor test_data, test_label in test_dataloader:\n\n    test_label = test_label[0].to(device)\n    mask = test_data['attention_mask'][0].to(device)\n\n    input_id = test_data['input_ids'][0].to(device)\n\n    loss, logits = model(input_id, mask, test_label)\n\n    logits_clean = logits[0][test_label != -100]\n    label_clean = test_label[test_label != -100]\n\n    predictions = logits_clean.argmax(dim=1)  \n    \n    # obtaining the label from indices \n    pred_ner = [ids_to_labels[pred.cpu().detach().numpy().item()]  for pred in predictions]\n\n    # print(test_data['input_ids'][0].shape)\n    # print(tokenizer.decode(test_data['input_ids'][0][0]))\n    predicted_labels.append(pred_ner)","metadata":{"id":"wbVtFucGA_ZT","execution":{"iopub.status.busy":"2022-05-26T00:23:41.84179Z","iopub.execute_input":"2022-05-26T00:23:41.842561Z","iopub.status.idle":"2022-05-26T00:23:44.857098Z","shell.execute_reply.started":"2022-05-26T00:23:41.842519Z","shell.execute_reply":"2022-05-26T00:23:44.855961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_labels = []\npred_labels = []\n\n# Now we iterate through each row of the test dataset and obtain the true store no. and predicted store no. \nfor index, row in df_test.iterrows():\n  sen, label = row['transaction_descriptor'], row['store_number']\n\n  s = sen.split(\" \")\n  print(s)\n  \n  try:\n    i = predicted_labels[index].index('B-sto')\n  except:\n    i = predicted_labels[index].index('O')\n\n  try:\n    store_no = re.findall(r'\\w+', s[i])\n    store_no = store_no[0].lstrip('0')\n  except:\n    store_no = s[i]\n\n  true_labels.append(label)\n  pred_labels.append(store_no)  \n  print(f'True label : {label}, predicted label : {store_no}')","metadata":{"id":"OncHtusrDmw-","outputId":"cffb80f3-a71c-468c-dba4-98052275dee5","execution":{"iopub.status.busy":"2022-05-26T00:23:54.199386Z","iopub.execute_input":"2022-05-26T00:23:54.200045Z","iopub.status.idle":"2022-05-26T00:23:54.260391Z","shell.execute_reply.started":"2022-05-26T00:23:54.200004Z","shell.execute_reply":"2022-05-26T00:23:54.259612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = 0\n\nfor i in range(len(true_labels)):\n    if(true_labels[i] == pred_labels[i]):\n      acc += 1\n\nprint(f'acc : {(acc/len(true_labels))*100}')","metadata":{"id":"2ihaHfl8SBmO","outputId":"137e9f0f-5744-411c-9ad8-b3ef08a8e3d3","execution":{"iopub.status.busy":"2022-05-26T00:24:07.360473Z","iopub.execute_input":"2022-05-26T00:24:07.360782Z","iopub.status.idle":"2022-05-26T00:24:07.36731Z","shell.execute_reply.started":"2022-05-26T00:24:07.360745Z","shell.execute_reply":"2022-05-26T00:24:07.366456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# End of notebook ","metadata":{"id":"SAU9NN0inwDC","execution":{"iopub.status.busy":"2022-05-25T05:47:34.476884Z","iopub.execute_input":"2022-05-25T05:47:34.477172Z","iopub.status.idle":"2022-05-25T05:47:34.48557Z","shell.execute_reply.started":"2022-05-25T05:47:34.477108Z","shell.execute_reply":"2022-05-25T05:47:34.484674Z"},"trusted":true},"execution_count":null,"outputs":[]}]}